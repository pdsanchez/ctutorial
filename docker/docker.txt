Libro:

Imagen útil: gliderlabs/logspout

Instalar Docker en Linux
--------------------------

Usar el script de https://get.docker.com para instalar Docker automáticamente. Dos opciones:
$ wget -qO- https://get.docker.com/ | sh
$ curl -sSL https://get.docker.com | sh 

Para poder trabajar con Docker sin sudo en Ubuntu, crearemos el grupo docker (si no existe ya) y se añadirá al usuario actual. Será necesario hacer un log out y logarse de nuevo:
$ sudo usermod -aG docker

Reiniciar el servicio Docker en Ubuntu:
$ sudo service docker restart

Chequear:
$ docker version
$ docker --version

Si el demonio Docker no se ejecuta o el cliente no puede acceder a él, intentaremos arrancar el demonio manualmente:
$ sudo docker daemon

Test Docker:
$ docker run debian echo "Hello World"
El comando 'run' lanza el contenedor. El argumento 'debian' es el nombre de la imagen a usar. 
Docker genera el contenedor a partir de la imagen y ejecuta en él el comando 'echo "Hello World"'

$ docker run -i -t debian /bin/bash
Los flags -i y -t indican a Docker que queremos una sesión interactiva con una tty (terminal).
El comando '/bin/bash' provee el contenedor con una shell de bash.

Comandos básicos
----------------
$ docker run --name boris debian echo "Boo"
Contenedor con nombre boris (en lugar de cientifico aleatorio)
$ docker run -h CONTAINER -i -t debian /bin/bash
Lanza un nuevo contenedor con el hostname especificado con el flag -h
$ docker ps
Muestra detalles de los contenedores que se están ejecutando actualmente.
$ docker ps -a
Lista todos los contenedores, incluidos los que estándetenidos (oficialmente llamados exited containers)
$ docker rm container_name
$ docker rm -v $(docker ps -aq -f status=exited)
Eliminar contenedor
Para evitar que se acumulen contenedores detenidos se puede poner el flag --rm con el comando 'run'. Esto eliminará el contenedor y el sistema de ficheros asociado cuando se cierre el contenedor.
$ docker inspect container_name
$ docker inspect container_name | grep IPAddress
$ docker inspect --format {{.NetworkSettings.IPAddress}} container_name
$ docker logs container_name
Lista todo lo que ha ocurrido en el contenedor especificado.

$ docker run -it --name cowsay --hostname cowsay debian bash
root@cowsay:/# apt-get update
root@cowsay:/# apt-get install -y cowsay fortune
root@cowsay:/# /usr/games/fortune | /usr/games/cowsay
root@cowsay:/# exit
Esto lanza un contenedor e instala una aplicación. Para transformar este contenedor en una imagen podemos usar el comando 'commit'. No importa si el contenedor está ejecutandose o está parado.
Hay que pasar el nombre del contenedor (“cowsay”) un nombre para la imagen (“cowsayimage”) y el nombre del repositorio donde se guardará (“test”):
$ docker commit cowsay test/cowsayimage
d1795abbc71e14db39d24628ab335c58b0b45458060d1973af7acf113a0ce61d
Esto retorna el unique ID de la imagen creada. Para ejecutarla:
$ docker run test/cowsayimage /usr/games/cowsay "Moo"
El problema de este sistema es que si queremos cambiar algo, hay que empezar de cero. Por esto lo mejor es usar unDockerfile para automatizar la creación de la imagen.

Crear imagen a partir del Dockerfile
El Dockerfile es un fichero de texto que contiene un conjunto de pasos que se usan para crear una imagen.
Empezaremos creando un directorio y un fichero:
$ mkdir cowsay
$ cd cowsay
$ touch Dockerfile
Insertaremos el siguiente contenido en el Dockerfile:
FROM debian:wheezy
RUN apt-get update && apt-get install -y cowsay fortune
La instrucción FROM especifica la imagen base que usaremos (debian, versión “wheezy”). Todos los Dockerfiles deben tener la instrucción FROM como la primera instrucción.
La instrucción RUN especidica un comando shell a ejecutar en la imagen (en este caso instalar cowsay y fortune)

Ahora podemos construir la imagen desde el directorio en el que se encuentra el Dockerfile:
$ docker build -t test/cowsay-dockerfile .
Y ejecutamos la imagen igual que antes:
$ docker run test/cowsay-dockerfile /usr/games/cowsay "Moo"

Images, Containers, and the Union File System
---------------------------------------------
UFS (Union File System - union mount) Permite que múltiples sistemas de ficheros se solapen de modo que para el usuario aparece como un único sistema de ficheros. Los directorios pueden contener ficheros procedentes de múltiples sistemas de ficheros, pero si dos ficheros tienen exactamente el mismo path, el último fichero montado ocultará a los anteriores.
Docker soporta varias implementaciones de UFS (AUFS, Overlay, devicemapper, BTRFS, ZFS) La implementación usada depende del sistema y se puede comprobar con 'docker info' (aparecerá especificado como “Storage Driver”)
Las imagenes en Docker se forman a partir de múltiples capas. Cada una de ellas es un sistema de ficheros de sólo lectura. Se crea una capa por cada instrucción del Dockerfile, y se sitúa encima de las capas anteriores.
Cuando se crea un contenedor a partir de una imagen, el motor de Docker toma la imagen y añade un sistema de ficheros de lectura encima de todo (también se configura la dirección IP, el nombre, ID y los límites de los recursos)
Las capas innecesarias consumen recursos de las imágenes (y los UFS pueden tener límites de capas). Por tanto los Dockerfiles tratan de minimizar el número de capas especificando varios comandos UNIX en una única instrucción RUN.

Un contenedor presenta varios estados: created, restarting, running, paused, or exited.
“created” -- contenedor inicializado con el commando 'create', pero aún no ha arrancado.
“stopped” -- indica que no hay procesos ejecutándose en el contenedor. Un contenedor en este estado no es igual que su imagen, ya que mantiene los cambios de configuración (dirección IP, metadatos, sistema de ficheros, runtime configuration)
"exited" -- un contenedor se cierra cuando se cierran sus procesos principales.
"restarting" -- no es frecuente. Ocurre cuando el motor Docker trata de reiniciar un contenedor fallido.

Para facilitar las cosas podemos usar la instrucción ENTRYPOINT en el Dockerfile. Esta instrucción permite especificar un ejecutable que se usa para manejar argumentos que se pasarán al ejecutar "docker run".
Añadimos la siguiente línea al final del Dockerfile:
ENTRYPOINT ["/usr/games/cowsay"]
Ahora podemos reconstruir y ejecutar la imagen sin necesidad de especificar el comando 'cowsay':
$ docker build -t test/cowsay-dockerfile .
$ docker run test/cowsay-dockerfile "Moo"

Pero ahora perdemos la capacidad de usar el comando 'fortune' en el contenedor como entrada de 'cowsay'. Para solucionarlo tenemos que añadir nuestro propio script para ENTRYPOINT, lo cual es común al crear Dockerfiles.
Creamos un entrypoint.sh en el mismo directorio que el Dockerfile y con lo siguiente:
#!/bin/bash
if [ $# -eq 0 ]; then
  /usr/games/fortune | /usr/games/cowsay
else
  /usr/games/cowsay "$@"
fi
Hacemos el fichero ejecutable:
$ chmod +x entrypoint.sh

Este script envía la salida de 'fortune' a 'cowsay' cuando no se usan argumentos. Con argumentos se llama a 'cowsay' con esos argumentos. 
En el Dockerfile añadiremos el script y lo llamaremos con la instrucción ENTRYPOINT:
FROM debian
RUN apt-get update && apt-get install -y cowsay fortune
COPY entrypoint.sh /
ENTRYPOINT ["/entrypoint.sh"]
La instrucción COPY copia el fichero especificado desde el host al sistema de ficheros de la imagen (COPY host_file destination_path)
Probamos:
$ docker build -t test/cowsay-dockerfile .
$ docker run test/cowsay-dockerfile
$ docker run test/cowsay-dockerfile HOLA

Image Namespaces: evitan confusiones en los usuarios sobre la procedencia de la imagen.
Existen tres namespaces a los que pueden pertenecer las imágenes de Docker:
• Nombres precedidos por un string y / (por ejemplo amouat/revealjs). Pertenencen al namespace "user".
Son imágenes del Docker Hub subidas por un usuario (en el ejemplo la imagen revealjs la ha subido el usuario amouat)
• Nombres sin prefijos o / (por ejemplo debian, ubuntu). Pertenencen al namespace "root" controlado por Docker Inc. y reservado para imágenes oficiales de software y distribuciones. Son curadas por Docker, aunque las mantienen los proveedores.
• Nombres precedidos por un hostname o IP (por ejemplo localhost:5000/wordpress). Son imágenes almacenadas en otros registros diferentes a Docker Hub (self-hosted registries de organizaciones, competidores de Hub como quay.io

Usar una imagen oficial de Redis (key-value store):
$ docker pull redis
Arrancar el contenedor Redis con -d (en background)
$ docker run --name myredis -d redis
Con -d se devuelve el ID del contenedor y se sale, aunque el contenedor queda ejecutándose en background (con 'docker logs' se pueden ver las salidas del contenedor)
¿Y cómo lo usamos? Necesitamos conectar con la base de datos de alguna manera. Usaremos redis-cli, pero en lugar de instalarlo en el host lanzaremos un nuevo contenedor que ejecute redis-cli, y enlazaremos los dos contenedores.
$ docker run --rm -it --link myredis:redis redis /bin/bash
root@ca38735c5747:/data# redis-cli -h redis -p 6379
redis:6379> ping
PONG
redis:6379> set "abc" 123
OK
redis:6379> get "abc"
"123"
redis:6379> exit
root@ca38735c5747:/data# exit
exit

El argumento '--link myredis:redis' indica a Docker que queremos conectar el contenedor nuevo al contenedor existente “myredis”, y que queremos referirnos a él con el nombre “redis” dentro del nuevo contenedor.
Para esto Docker configura una entrada para "redis" en /etc/hosts dentro del contenedor, apuntando a la dirección IP de “myredis”. Esto nos permite usar el hostname "redis" en la redis-cli en lugar de pasar o descubrir de alguna manera la dirección IP del contenedor Redis.
El comando 'ping' de Redis permite verificar que estamos conectados a Redis. Después añadimos y recuperamos datos.
Pero, ¿cómo persistimos y recuperamos nuestros datos?
Para esto no usaremos el sistema de ficheros estandar del contenedor. Necesitamos algo que permita compartir datos entre el host y los contenedores.
Docker soluciona esto mediante los volúmenes, ficheros o directorios que se montan en el sistema de ficheros del host directamente. Hay dos modos de declarar un directorio como volumen, usando la instrucción VOLUME en el Dockerfile o con el flag -v en 'docker run'. 
Por ejemplo:
VOLUME /data
$ docker run -v /data test/webserver
Ambos crean un volumen /data en el contenedor.

Por defecto, el directorio o fichero se montará en el host, dentro del directorio de instalación de Docker (normalmente /var/lib/docker/). Es posible especificar el directorio del host a usar desde 'docker run' (por ejemplo, docker run -d -v /host/dir:/container/dir test/webserver ).
Desde el Dockerfile esto no es posible por razones de portabilidad y seguridad (el fichero o directorio puede no existir en otros sistemas, o los contenedores pueden no tener permisos para montar ciertos ficheros o directorios)

En el ejemplo de Redis podemos hacer lo siguiente para crear backups (asumimos que el contenedor 'myredis' está ejecutándose):
$ docker run --rm -it --link myredis:redis redis /bin/bash
root@09a1c4abf81f:/data# redis-cli -h redis -p 6379
redis:6379> set "persistence" "test"
OK
redis:6379> save
OK
redis:6379> exit
root@09a1c4abf81f:/data# exit
exit
$ docker run --rm --volumes-from myredis -v $(pwd)/backup:/backup \
debian cp /data/dump.rdb /backup/
$ ls backup
dump.rdb

Hemos usado el argumento -v para montar un directorio conocido en el host y el argumento --volumes-from para conectar el nuevo contenedor a la carpeta de la base de datos Redis.
Una vez que se termina con el contenedor 'myredis', se puede detener y borrar:
$ docker stop myredis
myredis
$ docker rm -v myredis
myredis
Y se pueden eliminar todos los contenedores restantes con:
$ docker rm $(docker ps -aq)
45e404caa093
e4b31d0550cd
7a24491027fc
...


Surrounding Technologies
------------------------
El motor Docker y Docker Hub no son una solución completa para trabajar con contenedores. Otras tecnologías son:
Swarm -- Solución para clusters. Swarm agrupa varios host Docker, permitiendo al usuario tratarlos como un único recurso.
Compose -- Permite construir y ejecutar aplicaciones formadas por múltiples contenedores. Se usa principalmente en desarrollo y testeo (no en producción)
Machine -- Instala y configura hosts de Docker en recursos locales o remotos. También configura el cliente Docker, facilitando el cambio de entorno.
Kitematic -- GUI para Mac OS y Windows que permite ejecutar y manejar contenedores.
Docker Trusted Registry -- Solución para almacenar y manejar imágenes (versión local de Docker Hub que permite integrarse con una infraestructura de seguridad existente). A través de la consola administrativa se manejan logs, métricas, RBAC (Role-Based Access Control). No es open-source.
Alternativas son CoreOS Enterprise Registry and Artifactory de JFrog.
Networking -- Crear redes de contenedores que engloben varios hosts no es trivial. Existen varias soluciones (Weave y Proyecto Calico).
La solución de Docker se denomina Overlay. Los usuarios serán capaces de reemplazar el driver de Overlay por otras soluciones usando el networking plugin framework de Docker. Docker también soporta plugins de volúmenes para integrarse con otros sistemas de almacenamiento (Flocker, una herramienta de migración y gestión de datos multihost, y GlusterFS para almacenamiento distribuido)
Service discovery -- Cuando se crea un contenedor, se necesita algún modo de encontrar los otros servicios con los que tiene que comunicarse, los cuales normalmente se están ejecutando en otros contenedores. Como a los contenedores se les asigna la dirección IP dinámicamente, esto no es un problema trivial en sistemas grandes. Algunas soluciones incluyen Consul, Registrator, SkyDNS.
Orchestration and cluster management -- Cuando se despliegan contenedores grandes, es necesaria alguna herramienta para monitorizar y gestionar el sistema. Cada contenedor nuevo debe situarse en un host, monitorizarse y actualizarse. El sistema debe responder a fallos o cambios en la carga moviendo, arrancando o deteniendo los contenedores apropiadamente. Algunas soluciones son Kubernetes de Google, Marathon (un frame‐
work para Mesos), CoreOS de Fleet, y la herramienta de Docker, Swarm.

Connecting Containers to the World
----------------------------------
Say you’re running a web server inside a container. How do you provide the outside
world with access? The answer is to “publish” ports with the -p or -P commands. This
command forwards ports on the host to the container. For example:
$ docker run -d -p 8000:80 nginx
af9038e18360002ef3f3658f16094dadd4928c4b3e88e347c9a746b131db5444
$ curl localhost:8000
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
The -p 8000:80 argument has told Docker to forward port 8000 on the host to port
80 in the container. Alternatively, the -P argument can be used to tell Docker to auto‐
matically select a free port to forward to on the host. For example:
$ ID=$(docker run -d -P nginx)
$ docker port $ID 80
0.0.0.0:32771
$ curl localhost:32771
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
The primary advantage of the -P command is that you are no longer responsible for
keeping track of allocated ports, which becomes important if you have several con‐
tainers publishing ports. In these cases you can use the docker port command to
discover the port allocated by Docker.

Linking Containers
Docker links are the simplest way to allow containers on the same host to talk to each
other. When using the default Docker networking model, communication between
containers will be over an internal Docker network, meaning communications are
not exposed to the host network.
Docker Networking Changes
In future versions of Docker (likely 1.9 and on), the idiomatic way
to network containers will be to “publish services,” rather than link
containers. 
Links are initialized by giving the argument --link CONTAINER:ALIAS to docker
run , where CONTAINER is the name of the link container 3 and ALIAS is a local name
used inside the master container to refer to the link container.
Using Docker links will also add the alias and the link container ID to /etc/hosts on
the master container, allowing the link container to be addressed by name from the
master container.
In addition, Docker will set a bunch of environment variables inside the master con‐
tainer that are designed to make it easy to talk to the link container. For example, if
we create and link to a Redis container:
$ docker run -d --name myredis redis
c9148dee046a6fefac48806cd8ec0ce85492b71f25e97aae9a1a75027b1c8423
$ docker run --link myredis:redis debian env
ATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=f015d58d53b5
REDIS_PORT=tcp://172.17.0.22:6379
REDIS_PORT_6379_TCP=tcp://172.17.0.22:6379
REDIS_PORT_6379_TCP_ADDR=172.17.0.22
REDIS_PORT_6379_TCP_PORT=6379
REDIS_PORT_6379_TCP_PROTO=tcp
REDIS_NAME=/distracted_rosalind/redis
REDIS_ENV_REDIS_VERSION=3.0.3
REDIS_ENV_REDIS_DOWNLOAD_URL=http://download.redis.io/releases/redis-3.0.3.tar.gz
REDIS_ENV_REDIS_DOWNLOAD_SHA1=0e2d7707327986ae652df717059354b358b83358
HOME=/root
we can see that Docker has set up environment variables prefixed with REDIS_PORT ,
that contain information on how to connect to the container. Most of these seem
somewhat redundant, as the information in the value is already contained in the vari‐
able name. Nevertheless, they are useful as a form of documentation if nothing else.
Docker has also imported environment variables from the linked container, which it
has prefixed with REDIS_ENV . While this functionality can be very useful, it’s impor‐
tant to be aware that this happens if you use environment variables to store secrets
such as API tokens or database passwords.
By default, containers will be able to talk to each other whether or not they have been
explicitly linked. If you want to prevent containers that haven’t been linked from
communicating, use the arguments --icc=false and --iptables when starting the
Docker daemon. Now when containers are linked, Docker will set up Iptables rules to
allow the containers to communicate on any ports that have been declared as
exposed.
Unfortunately, Docker links as they stand have several shortcomings. Perhaps most
significantly, they are static—although links should survive container restarts, they
aren’t updated if the linked container replaced. Also, the link container must be
started before the master container, meaning you can’t have bidirectional links.


Managing Data with Volumes and Data Containers
To recap, Docker volumes are directories 4 that are not part of the container’s UFS (see
“Images, Containers, and the Union File System”)—they are just normal directories
on the host that are bind mounted (see Bind Mounting) into the container.
There are three 5 different ways to initialize volumes, and it’s important to understand
the differences between the methods. First, we can declare a volume at runtime with
the -v flag:
$ docker run -it --name container-test -h CONTAINER -v /data debian /bin/bash
root@CONTAINER:/# ls /data
root@CONTAINER:/#
This will make the directory /data inside the container into a volume. Any files the
image held inside the /data directory will be copied into the volume. We can find out
where the volume lives on the host by running docker inspect on the host from a
new shell:
$ docker inspect -f {{.Mounts}} container-test
[{5cad... /mnt/sda1/var/lib/docker/volumes/5cad.../_data /data local
true}]
In this case, the volume /data/ in the container is simply a link to the direc‐
tory /var/lib/docker/volumes/5cad.../_data on the host. To prove this, we can add a
file into the directory on the host: 6
$ sudo touch /var/lib/docker/volumes/5cad.../_data/test-file
And you should immediately be able to see from inside the container:
$ root@CONTAINER:/# ls /data
test-file
The second way to set up a volume is by using the VOLUME instruction in a Dockerfile:
FROM debian:wheezy
VOLUME /data
This has exactly the same effect as specifying -v /data to docker run .

Setting Volume Permissions in Dockerfiles
You will often need to set the permissions and ownership on a volume or initialize a
volume with some default data or configuration files. The key point to be aware of
here is that any instruction after the VOLUME instruction in a Dockerfile will not be
able to make changes to that volume. For example, the following Dockerfile will not
work as expected:
FROM debian:wheezy
RUN useradd foo
VOLUME /data
RUN touch /data/x
RUN chown -R foo:foo /data
We want the touch and chown commands to run on the image’s filesystem, but they
will actually run inside the volume of a temporary container used to create the layer
(refer back to “How Images Get Built” for more details). This volume will be removed
once the commands complete, rendering the instruction pointless.
The following Dockerfile will work:
FROM debian:wheezy
RUN useradd foo
RUN mkdir /data && touch /data/x
RUN chown -R foo:foo /data
VOLUME /data
When a container is started from this image, Docker will copy any files from the vol‐
ume directory in the image into the container’s volume. This won’t happen if you
specify a host directory for the volume (so that host files aren’t accidentally overwrit‐
ten).
If for some reason you can’t set permissions and ownership in a RUN instruction, you
will have to do so using a CMD or ENTRYPOINT script that runs after container creation.

The third 7 way is to extend the -v argument to docker run with an explicit directory
to bind to on the host using the format -v HOST_DIR:CONTAINER_DIR . This can’t be
done from a Dockerfile (it would be nonportable and a security risk). For example:
$ docker run -v /home/adrian/data:/data debian ls /data
This will mount the directory /home/adrian/data on the host as /data inside the con‐
tainer. Any files already existing in the /home/adrian/data directory will be available
inside the container. If the /data directory already exists in the container, its contents
will be hidden by the volume. Unlike the other invocations, no files from the image
will be copied into the volume, and the volume won’t be deleted by Docker (i.e.,
docker rm -v will not remove a volume that is mounted at a user-chosen directory).

Bind Mounting
When a specific host directory is used in a volume (the -v
HOST_DIR:CONTAINER_DIR syntax), it is often referred to as bind
mounting. This is somewhat misleading, as all volumes are techni‐
cally bind mounted—the difference is that the mount point is made
explicit rather than hidden in a directory owned by Docker.


Sharing Data
The -v HOST_DIR:CONTAINER_DIR syntax is very useful for sharing files between the
host and one or more containers. For example, configuration files can be kept on the
host and mounted into containers built from generic images.
We can also share data between containers by using the --volumes-from CONTAINER
argument with docker run . For example, we can create a new container that has
access to the volumes from the container in our previous example like so:
$ docker run -it -h NEWCONTAINER --volumes-from container-test debian /bin/bash
root@NEWCONTAINER:/# ls /data
test-file
root@NEWCONTAINER:/#
It’s important to note that this works whether or not the container holding the vol‐
umes ( container-test in this case) is currently running. As long as at least one exist‐
ing container links to a volume, it won’t be deleted.


Data Containers
A common practice is to create data containers—containers whose sole purpose is to
share data between other containers. The main benefit of this approach is that it pro‐
vides a handy namespace for volumes that can be easily loaded using the --volumes-
from command.
For example, we can create a data container for a PostgreSQL database with the fol‐
lowing command:
$ docker run --name dbdata postgres echo "Data-only container for postgres"
This will create a container from the postgres image and initialize any volumes
defined in the image before running the echo command and exiting. 8 There’s no need
to leave data containers running, since doing so would just be a waste of resources.
We can then use this volume from other containers with the --volumes-from argu‐
ment. For example:
$ docker run -d --volumes-from dbdata --name db1 postgres

Images for Data Containers
There’s normally no need to use a “minimal image” such as busy
box or scratch for the data container. Just use the same image that
is used for the container consuming the data. For example, use the
postgres image to create a data container to be used with the Post‐
gres database.
Using the same image doesn’t take up any extra space—you must
already have downloaded or created the image for the consumer. It
also gives the image a chance to seed the container with any initial
data and ensures permissions are set up correctly.

Deleting volumes
Volumes are only deleted if:
• the container was deleted with docker rm -v , or
• the --rm flag was provided to docker run
and:
• no existing container links to the volume
• no host directory was specified for the volume (the -v HOST_DIR:CONTAINER_DIR
syntax was not used)
At the moment, this means that unless you are very careful about always running
your containers like this, you are likely to have orphan files and directories in your
Docker installation directory and no easy way of telling what they represent. Docker
is working on a top-level “volume” command that will allow you to list, create,
inspect, and remove volumes independent of containers. This is expected to land in
1.9, which should be out by the time this book is published.



Common Docker Commands
I have not specified in detail the arguments and
syntax of the various commands (with the exception of docker run ). Refer to the in-
built help for this, which can be accessed by giving the --help argument to any com‐
mand or via the docker help command.

Docker Boolean Flags
In most Unix command-line tools, you will find flags that don’t
take a value, such as -l in ls -l . Since these flags are either set or
not set, Docker considers these to be boolean flags and—unlike
most other tools—supports explictly supplying a boolean value flag
(i.e., it will accept both -f=true and -f ). In addition (and this is
where things get confusing), you can have both default true and
default false flags. Unlike default false, default true flags are consid‐
ered to be set if unspecified. Specifying a flag without an argument
has the same effect as setting it to true—a default true flag is not
unset by an argument with a value; the only way a default true flag
can be unset is by explicitly setting it to false (e.g., -f=false ).
To find out if a flag is default true or default false, refer to docker
help for the command. For example:
$ docker logs --help
...
-f, --follow=false
--help=false
-t, --timestamps=false
...
Follow log output
Print usage
Show timestamps
shows that the -f , --help , and -t arguments are all default false.
To give a couple of concrete examples, consider the default true --
sig-proxy argument to docker run . The only way to turn this
argument off is by explicitly setting it false. For example:
$ docker run --sig-proxy=false ...
All of the following are equivalent:
$ docker run --sig-proxy=true ...
$ docker run --sig-proxy ...
$ docker run ...
In the case of a default false argument, such as --read-only , the
following will set it to true:
$ docker run --read-only=true
$ docker run --read-only
Leaving it unspecified or explicitly setting to false are equivalent.
This also leads to some quirky behavior with flags that normally
short-circuit logic (e.g., docker ps --help=false will work as
normal without printing the help message).

The run Command
---------------
Launchs new containers.

The following options control the lifecycle of the container and its basic mode of
operation:
-a, --attach
Attaches the given stream ( STDOUT , etc.) to the terminal. If unspecified, both stdout and stderr are attached. If unspecified and the container is started in interactive mode ( -i ), stdin is also attached.
Incompatible with -d
-d, --detach
Runs the container in “detached” mode. The command will run the container in the background and return the container ID.
-i, --interactive
Keeps stdin open (even when it’s not attached). Generally used with -t to start an interactive container session.
--restart
Configures when Docker will attempt to restart an exited container.
--rm
Automatically removes the container when it exits. Cannot be used with -d .
-t, --tty
Allocates a pseudo-TTY. Normally used with -i to start an interactive container.

The following options allow setting of container names and variables:
-e, --env
Sets environment variables inside the container. For example:
$ docker run -e var1=val -e var2="val 2" debian env
Also note the --env-file option for passing variables in via a file.
-h, --hostname
Sets the container’s unix host name to NAME
--name NAME
Assigns the name NAME to the container. The name can then be used to address
the container in other Docker commands.

The following options allow the user to set up volumes:
-v, --volume
There are two forms of the argument to set up a volume (a file or directory
within a container that is part of the native host filesystem, not the container’s
union file system). The first form only specifies the directory within the con‐
tainer and will bind to a host directory of Docker’s choosing. The second form
specifies the host directory to bind to.
--volumes-from
Mounts volumes from the specified container. Often used in association with
data containers (see “Data Containers”).

There are several options affecting networking:
--expose
Equivalent of Dockerfile EXPOSE instruction. Identifies the port or port range as
being used in the container but does not open the port. Only really makes sense
in association with -P and when linking containers.
--link
Sets up a private network interface to the specified container. See “Linking Con‐
tainers” for more information.
-p, --publish
“Publishes” a port on the container, making it accessible from the host. If the host
port is not defined, a random high-numbered port will chosen, which can be dis‐
covered by using the docker port command. The host interface on which to
expose the port may also be specified.
-P, --publish-all
Publish all exposed ports on the container to the host. A random high-numbered
port will be chosen for each exposed port. The docker port command can be
used to see the mapping.

The following options directly override Dockerfile settings:
--entrypoint
Sets the entrypoint for the container to the given argument, overriding any ENTRY
POINT instruction in the Dockerfile.
-u, --user
Sets the user that commands are run under. May be specified as a username or
UID. Overrides USER instruction in Dockerfile.
-w, --workdir
Sets the working directory in the container to the provided path. Overrides any
value in the Dockerfile.

Managing Containers
-------------------
In addition to docker run , the following docker commands are used to manage con‐
tainers during their lifecyle:
docker attach [OPTIONS] CONTAINER
The attach command allows the user to view or interact with the main process
inside the container. For example:
$ ID=$(docker run -d debian sh -c "while true; do echo 'tick'; sleep 1; done;")
$ docker attach $ID
tick
tick
tick
tick
Note that using CTRL-C to quit will end the process and cause the container to exit.
docker create
Creates a container from an image but does not start it. Takes most of the same
arguments as docker run . To start the container, use docker start .
docker cp
Copies files and directories between a container and the host.
docker exec
Runs a command inside a container. Can be used to perform maintenance tasks
or as a replacement for ssh to log in to a container.
For example:
$ ID=$(docker run -d debian sh -c "while true; do sleep 1; done;")
$ docker exec $ID echo "Hello"
Hello
$ docker exec -it $ID /bin/bash
root@5c6c32041d68:/# ls
bin dev home lib64 mnt proc run
selinux sys usr
boot etc lib
media opt root sbin srv
tmp var
root@5c6c32041d68:/# exit
exit
docker kill
Sends a signal to the main process (PID 1) in a container. By default, sends a
SIGKILL , which will cause the container to exit immediately. Alternatively, the
signal can be specified with the -s argument. The container ID is returned.
For example:
$ ID=$(docker run -d debian bash -c \
"trap 'echo caught' SIGTRAP; while true; do sleep 1; done;")
$ docker kill -s SIGTRAP $ID
e33da73c275b56e734a4bbbefc0b41f6ba84967d09ba08314edd860ebd2da86c
$ docker logs $ID
caught
$ docker kill $ID
e33da73c275b56e734a4bbbefc0b41f6ba84967d09ba08314edd860ebd2da86c
docker pause
Suspends all processes inside the given container. The processes do not receive
any signal that they are being suspended and consequently cannot shut down or
clean up. The processes can be restarted with docker unpause . docker pause
uses the Linux cgroups freezer functionality internally. This command contrasts
with docker stop , which stops the processes and sends signals observable by the
processes.
docker restart
Restarts one or more containers. Roughly equivalent to calling docker stop fol‐
lowed by docker start on the containers. Takes an optional argument -t that
specifies the amount of time to wait for the container to shut down before it is
killed with a SIGTERM .
docker rm
Removes one or more containers. Returns the names or IDs of succesfully
deleted containers. By default, docker rm will not remove any volumes. The -f
argument can be used to remove running containers, and the -v argument will
remove volumes created by the container (as long as they aren’t bind mounted or
in use by another container).
For example, to delete all stopped containers:
$ docker rm $(docker ps -aq)
b7a4e94253b3
e33da73c275b
f47074b60757
docker start
Starts a stopped container (or containers). Can be used to restart a container that
has exited or to start a container that has been created with docker create but
never launched.
docker stop
Stops (but does not remove) one or more containers. After calling docker stop
on a container, it will transition to the “exited” state. Takes an optional argument
-t which specifies the amount of time to wait for the container to shutdown
before it is killed with a SIGTERM .
docker unpause
Restarts a container previously paused with docker pause .

Detaching from Containers
When attached to a Docker container, either by starting it in inter‐
active mode or attaching to it with docker attach , you will stop
the container if you try to disconnect with CTRL-C. Instead, if you
use CTRL-P CTRL-Q you can detach from the container without
stopping it.
This code will only work when attached in interactive mode with a
TTY (i.e., using both the -i and -t flags).

Docker Info
-----------
The following subcommands can be used to get more information on the Docker installation and usage:
$ docker info  ---  Prints various information on the Docker system and host.
$ docker help  ---  Prints usage and help information for the given subcommand. Identical to running a command with the --help flag.
$ docker version  ---  Prints Docker version information for client and server as well as the version of Go used in compilation.

Container Info
--------------
The following commands provide more information on running and stopped containers.
$ docker diff  ---  Shows changes made to the containers filesystem compared to the image it was launched from. For example:
$ ID=$(docker run -d debian touch /NEW-FILE)
$ docker diff $ID
A /NEW-FILE

$ docker events  ---  Prints real-time events from the daemon. Use CTRL-C to quit. 
$ docker inspect  ---  Provides detailed information on given containers or images. The information
includes most configuration information and covers network settings and volume mappings. The command can take one argument, -f , which is used to supply a Go template that can be used to format and filter the output.
$ docker logs  ---  Outputs the “logs” for a container. This is simply everything that has been written to STDERR or STDOUT inside the container. 
$ docker port  ---  Lists the exposed port mappings for the given container. Can optionally be given
the internal container port and protocol to look up. Often used after docker run -P <image> to discover the assigned ports.
$ ID=$(docker run -P -d redis)
$ docker port $ID
6379/tcp -> 0.0.0.0:32768
$ docker port $ID 6379
0.0.0.0:32768
$ docker port $ID 6379/tcp
0.0.0.0:32768

$ docker ps  ---  Provides high-level information on current containers, such as the name, ID, and
status. Takes a lot of different arguments, notably -a for getting all containers,
not just running ones. Also note the -q argument, which only returns the con‐
tainer IDs and is very useful as input to other commands such as docker rm .
$ docker top  ---  Provides information on the running processes inside a given container. In effect,
this command runs the UNIX ps utility on the host and filters for processes in
the given container. The command can be given the same arguments the ps util‐
ity and defaults to -ef (but be careful to make sure the PID field is still in the
output).
For example:
$ ID=$(docker run -d redis)
$ docker top $ID
UID	PID	PPID C STIME TTY TIME	CMD
999 9243 1836 0 15:44 ? 00:00:00 redis-server *:6379
$ ps -f -u 999
UID PID	PPID C STIME TTY	TIME CMD
999 9243 1836 0 15:44 ? 00:00:00 redis-server *:6379
$ docker top $ID -axZ
LABEL	PID TTY STAT TIME COMMAND
docker-default 9243 ? Ssl 0:00 redis-server *:6379

Dealing with Images
-------------------
The following commands provide tools for creating and working with images:
$ docker build  ---  Builds an image from a Dockerfile
$ docker commit  ---  Creates an image from the specified container. While docker commit can be use‐
ful, it is generally preferable to create images using docker build , which is easily
repeatable. By default, containers are paused prior to commit, but this can be
turned off with the --pause=false argument. Takes -a and -m arguments for set‐
ting metadata.
For example:
$ ID=$(docker run -d redis touch /new-file)
$ docker commit -a "Joe Bloggs" -m "Comment" $ID commit:test
ac479108b0fa9a02a7fb290a22dacd5e20c867ec512d6813ed42e3517711a0cf
$ docker images commit
REPOSITORY TAG	IMAGE ID	CREATED	VIRTUAL SIZE
commit	test ac479108b0fa About a minute ago 111 MB
$ docker run commit:test ls /new-file
/new-file
$ docker export  ---  Exports the contents of the container’s filesystem as a tar archive on STDOUT . The
resulting archive can be loaded with docker import . Note that only the filesys‐
tem is exported; any metadata such as exported ports, CMD , and ENTRYPOINT set‐
tings will be lost. Also note that any volumes are not inlcuded in the export.
Contrast with docker save .
$ docker history  ---  Outputs information on each of the layers in an image.
$ docker images  ---  Provides a list of local images, including information such as repository name,
tag name, and size. By default, intermediate images (used in the creation of top-
level images) are not shown. The VIRTUAL SIZE is the total size of the image
including all underlying layers. As these layers may be shared with other images,
simply adding up the size of all images does not provide an accurate estimate of
disk usage. Also, images will appear multiple times if they have more than one
tag; different images can be discerned by comparing the ID. Takes several argu‐
ments; in particular, note -q , which only returns the image IDs and is useful as
input to other commands such as docker rmi .
For example:
$ docker images | head -4
REPOSITORY	TAG	IMAGE ID	CREATED	VIRTUAL SIZE
identidock_identidock latest 9fc66b46a2e6 26 hours ago 839.8 MB
redis	latest 868be653dea3 6 days ago	110.8 MB
containersol/pres-base latest 13919d434c95 2 weeks ago 401.8 MB
To remove all dangling images:
$ docker rmi $(docker images -q -f dangling=true)
Deleted: a9979d5ace9af55a562b8436ba66a1538357bc2e0e43765b406f2cf0388fe062
$ docker import  ---  Creates an image from an archive file containing a filesystem, such as that created
by docker export . The archive may be identified by a file path or URL or
streamed through STDIN (by using the - flag). Returns the ID of the newly cre‐
ated image. The image can be tagged by supplying a repository and tag name.
Note that an image built from import will only consist of a single layer and will
lose Docker configuration settings such as exposed ports and CMD values. Con‐
trast with docker load .
Example of “flattening” an image by exporting and importing:
$ docker export 35d171091d78 | docker import - flatten:test
5a9bc529af25e2cf6411c6d87442e0805c066b96e561fbd1935122f988086009
$ docker history flatten:test
IMAGE	CREATED	CREATED BY SIZE	COMMENT
981804b0c2b2 59 seconds ago	317.7 MB Imported from -
$ docker load  ---  Loads a repository from a tar archive passed via STDIN . The repository may con‐
tain several images and tags. Unlike docker import , the images will include his‐
tory and metadata. Suitable archive files are created by docker save , making
save and load a viable alternative to registries for distributing images and pro‐
ducing backups. See docker save for an example.
$ docker rmi  ---  Deletes the given image or images. Images are specified by ID or repository and
tag name. If a repository name is supplied but no tag name, the tag is assumed to
be latest . To delete images that exist in multiple repositories, specify that image
by ID and use the -f argument. You will need to run this once per repository.
$ docker save  ---  Saves the named images or repositories to a tar archive, which is streamed to
STDOUT (use -o to write to a file). Images can be specified by ID or as
repository:tag . If only a repository name is given, all images in that repository
will be saved to the archive, not just the latest tag. Can be used in conjunction
with docker load to distribute or back up images.
For example:
$ docker save -o /tmp/redis.tar redis:latest
$ docker rmi redis:latest
Untagged: redis:latest
Deleted: 868be653dea3ff6082b043c0f34b95bb180cc82ab14a18d9d6b8e27b7929762c
...
$ docker load -i /tmp/redis.tar
$ docker images redis
REPOSITORY	TAG	IMAGE ID CREATED VIRTUAL SIZE
redis	latest	0f3059144681 3 months ago	111 MB
$ docker tag  ---  Associates a repository and tag name with an image. The image can identified by
ID or repository and tag (the latest tag is assumed if none is given). If no tag is
given for the new name, latest is assumed.
For example:
$ docker tag faa2b75ce09a newname (1)
$ docker tag newname:latest amouat/newname (2)
$ docker tag newname:latest amouat/newname:newtag (3)
$ docker tag newname:latest myregistry.com:5000/newname:newtag (4)
(1) Adds the image with ID faa2b75ce09a to the repository newname , using the
tag latest as none was specified.
(2) Adds the newname:latest image to the amouat/newname repository, again
using the tag latest. This label is in a format suitable for pushing to the
Docker Hub, assuming the user is amouat .
(3) As above except using the tag newtag instead of latest .
(4) Adds the newname:latest image to the repository myregistry.com/newname
with the tag newtag . This label is in a format suitable for pushing to a registry
at
http://myregistry.com:5000.(((range="endofrange”,
startref="ix_04_docker_fundamentals-asciidoc23”)))(((range="endofrange”,
startref="ix_04_docker_fundamentals-asciidoc22”)))

Using the Registry
------------------
The following commands relate to using registries, including the Docker Hub. Be
aware the Docker saves credentials to the file .dockercfg in your home directory:
$ docker login  ---  Register with, or log in to, the given registry server. If no server is specified, it is
assumed to be the Docker Hub. The process will interactively ask for details if
required, or they can be supplied as arguments.
$ docker logout  ---  Logs out from a Docker registry. If no server is specified, it is assumed to be the
Docker Hub.
$ docker pull  ---  Downloads the given image from a registry. The registry is determined by the
image name and defaults to the Docker Hub. If no tag name is given, the image
tagged latest will be downloaded (if available). Use the -a argument to down‐
load all images from a repository.
$ docker push  ---  Pushes an image or repository to the registry. If no tag is given, this will push all
images in the repository to the registry, not just the one marked latest .
$ docker search  ---  Prints a list of public repositories on the Docker Hub matching the search term.
Limits results to 25 repositories. You can also filter by stars and automated
builds. In general, it’s easiest to use the website.

---------------------------------------------------------------------------------

Docker containers can be used in development, testing, and production (on a single-host, or on multiple hosts)
To make the most of Docker, it is important to adopt a DevOps approach. In particular, during
development, we will be thinking about how to run software in production, which
will ease the pain of deployment to a variety of environments.
Containers are not suited to building enterprise software monoliths with a release
cycle measured in weeks or months. Instead, we will naturally find ourselves taking
microservice approach and exploring techniques such as continuous deployment
where it is possible to safely push to production multiple times a day.
The advantage of containers, DevOps, microservices, and continuous delivery essen‐
tially comes down to the idea of a fast feedback loop. By iterating quicker, we can
develop, test, and validate systems of higher quality in shorter time periods.


Using Docker in Development
---------------------------
Se realizará un ejemplo de aplicación en Python y el framework web Flask. Docker manejará todas las dependencias (por tanto no hay que instalar Python o Flask en tu máquina)

Se creará un servidor web que devuelva “Hello World!”.
Creamos un directorio (identidock) con un subdirectorio (app) que tendrá el código Python (fichero identidock.py):
$ tree identidock/
identidock/
└── app
     └── identidock.py
1 directory, 1 file

identidock.py:
from flask import Flask
app = Flask(__name__) # Initializes Flask and sets up the application object.

@app.route('/') # Creates a route associated with the URL. Whenever this URL is requested, it will result in a call to the hello_world function.
def hello_world():
  return 'Hello World!\n'

if __name__ == '__main__':
  app.run(debug=True, host='0.0.0.0') # Creates a route associated with the URL. Whenever this URL is requested, it will
result in a call to the hello_world function.
Initializes the Python webserver. The use of 0.0.0.0 (instead of localhost or
127.0.0.1 ) as host argument binds to all network interfaces, which is needed to
allow the container to be accessed from the host or other containers. The if
statement on the line above ensures this line only executes when the file is called
as a standalone program and not when running as part of a larger application.

Now we need a container to put this code in and run it. In the identidock directory,
create a file called Dockerfile with the following contents:
FROM python:3.4
RUN pip install Flask==0.10.1
WORKDIR /app
COPY app /app
CMD ["python", "identidock.py"]
This Dockerfile uses an official Python image as a base, which contains a Python 3
installation. On top of this, it installs Flask and copies in our code. The CMD instruc‐
tion simply runs our identidock code.

Official Image Variants
Many of the official repositories for popular programming languages such as Python,
Go, and Ruby contain multiple images for different purposes. In addition to images
for different version numbers, you are likely to find one or both of the following:
slim
These images are cut-down versions of the standard images. Many common
packages and libraries will be missing. These are essential when you need to
reduce on image size for distribution but often require extra work installing and
maintaining packages already available in the standard image.
onbuild
These images use the Dockerfile ONBUILD instruction to delay execution of cer‐
tain commands until a new “child” image is built that inherits the onbuild image.
These commands are processed as part of the FROM instruction of the child image
and typically do things like copy over code and run a compile step. These images
can make it quicker and easier to get started with a language, but in the long-run,
they tend to be limiting and confusing. I would generally only recommend using
onbuild images when first exploring a repository.
For our example application, we are using a standard base image for Python 3 and not
one of these variants.

Now we can build and run our simple application:
$ cd identidock
$ docker build -t identidock .
...
$ docker run -d -p 5000:5000 identidock
0c75444e8f5f16dfe5aceb0aae074cc33dfc06f2d2fb6adb773ac51f20605aa4
Here I’ve passed the -d flag to docker run in order to start the container in the back‐
ground, but you can also omit it if you want to see output from the webserver. The -p
5000:5000 argument tells Docker we want to forward port 5000 in the container to
port 5000 on the host.
Now let’s test it out:
$ curl localhost:5000
Hello World!

Docker Machine IPs
Si se ejecuta Docker usando Docker machine (lo normal si se usa Docker en Mac o Windows), no se puede usar localhost como URL. Hay que usar la dirección IP de la VM que ejecuta Docker. El comando 'docker-machine ip' ayuda a automatizar esto:
$ curl $(docker-machine ip default):5000
Hello World!

El problema de este workflow es que cualquier cambio del código obliga a reconstruir la imagen y reiniciar el contenedor.
Para solucionarlo enlazamos el directorio del código fuente del host sobre el que está en el contenedor (bind mount).
El siguiente código detiene y elimina el último contenedor antes de arrancar otro nuevo con el código del directorio montado en /app:
$ docker stop $(docker ps -lq)
0c75444e8f5f
$ docker rm $(docker ps -lq)
$ docker run -d -p 5000:5000 -v "$(pwd)"/app:/app identidock
El argumento '-v $(pwd)/app:/app' monta el directori app en /app dentro del contenedor. Se sobreescribe el contenido de /app que haya en el contenedor (es writable, pero también se puede hacer read-only). Los argumentos de -v deben ser paths absolutos (en el ejemplo se usa $(pwd) para concatenar el directorio actual (así es más portable)

Bind Mounts
When a host directory is specified for a volume using the -v
HOST_DIR:CONTAINER_DIR argument to docker run , it is com‐
monly referred to as a “bind mount,” as it binds a folder (or file) on
the host to a folder (or file) inside the container. This is a little con‐
fusing, as all volumes are technically bind mounts, but we have to
do a little more work to find the folder on the host when it isn’t
specificed explicitly.
Note that the HOST_DIR always refers to the machine running the
Docker engine. If you are connected to a remote Docker daemon,
the path must exist on the remote machine. If you’re using a local
VM provisioned by Docker machine (which you will be if you
installed Docker via Toolbox), it will cross-mount your home
directory to make things easier during development.

Verificamos que continúa funcionando:
$ curl localhost:5000
Hello World!

Ahora se está usando el mismo directorio en el host y en el contenedor, en lugar de su propia copia de la imagen. Esto permite editar identidock.py para ver los cambios inmediatamente:
$ sed -i '' s/World/Docker/ app/identidock.py
$ curl localhost:5000
Hello Docker!

Ahora tenemos un entorno de desarrollo normal, excepto que las dependencias (el compilador de Python y las librerías) están encapsuladas en el contenedor.
El problema es que no se puede usar este contenedor en producción, principalmente porque usa el webserver por defecto de Flask que está pensado para desarrollo (es ineficiente e inseguro para producción). Un punto importante a la hora de adoptar Docker es minimizar las diferencias entre desarrollo y producción.

Wot? No virtualenv?
If you’re an experienced Python developer, you may be surprised that we’re not using
virtualenv to develop our application. virtualenv is an extremely useful tool for isolat‐
ing Python environments. It allows developers to have separate versions of Python
and supporting libraries for each application. Normally, it is essential and ubiquitous
in Python development.
When using containers, however, it is less useful, as we are already provided with an
isolated environment. If you’re used to virtualenv, you can certainly still use it inside a
container, but you are unlikely to see much benefit, unless you experience clashes
with other applications or libraries installed in the container.

uWSGI es un servidor de aplicaciones preparado para producción que se puede poner detrás de un webserver como nginx.
Para usar uWSGI en lugar del webserver por defecto de Flask, modificamos dos líneas del Dockerfile:
FROM python:3.4
RUN pip install Flask==0.10.1 uWSGI==2.0.8 # -----> Add uWSGI to the list of Python packages to install.
WORKDIR /app
COPY app /app
CMD ["uwsgi", "--http", "0.0.0.0:9090", "--wsgi-file", "/app/identidock.py", "--callable", "app", "--stats", "0.0.0.0:9191"] # ------> Create a new command to run uWSGI. Here we tell uWSGI to start an http
server listening on port 9090, running the app application from /app/identi‐
dock.py. It also starts a stats server on port 9191. We could alternatively have
overridden the CMD via the docker run command.

Build it and run it so that we can see the difference:
$ docker build -t identidock .
...
Successfully built 3133f91af597
$ docker run -d -p 9090:9090 -p 9191:9191 identidock
00d6fa65092cbd91a97b512334d8d4be624bf730fcb482d6e8aecc83b272f130
$ curl localhost:9090
Hello Docker!

If you now run docker logs with the container ID, you will see the logging informa‐
tion for uWSGI, confirming we are indeed using the uWSGI server. Also, we’ve asked
uWSGI to expose some stats, which you can see at http://localhost:9191. The Python
code that normally starts the default web server hasn’t been executed as it wasn’t run
directly from the command line.

The server is working correctly now, but there is still some housekeeping we should
do. If you examine the uWSGI logs, you’ll notice that the server is rightly complain‐
ing about being run as root. This is a pointless security leak we can easily fix in the
Dockerfile by specifying a user to run under. At the same time, we will explicitly
declare the ports the container listens on:

FROM python:3.4
RUN groupadd -r uwsgi && useradd -r -g uwsgi uwsgi # ----> Creates the uwsgi user and group in a normal Unix fashion.
RUN pip install Flask==0.10.1 uWSGI==2.0.8
WORKDIR /app
COPY app /app
EXPOSE 9090 9191 # ----> Uses the EXPOSE instruction to declare the ports accessible to the host and other containers.
USER uwsgi # ----> Sets the user for all the following lines (including CMD and ENTRYPOINT ) to be uwsgi.
CMD ["uwsgi", "--http", "0.0.0.0:9090", "--wsgi-file", "/app/identidock.py", "--callable", "app", "--stats", "0.0.0.0:9191"]
To explain the new lines:

Users and Groups Inside Containers
The Linux kernel uses UIDs and GIDs to
identify users and detemine their access rights. Mapping UIDs and
GIDs to identifiers is handled in userspace by the OS. Because of
this, UIDs in the container are the same as UIDs on the host, but
users and groups created inside containers do not propogate to the
host. A side effect of this is that access permissions can get confus‐
ing; files can appear to be owned by different users inside and out‐
side of containers. For example, note the changing owner of the
following file:
$ ls -l test-file
-rw-r--r-- 1 docker staff 0 Dec 28 18:26 test-file
$ docker run -it -v $(pwd)/test-file:/test-file
debian bash
root@e877f924ea27:/# ls -l test-file
-rw-r--r-- 1 1000 staff 0 Dec 28 18:26 test-file
root@e877f924ea27:/# useradd -r test-user
root@e877f924ea27:/# chown test-user test-file
root@e877f924ea27:/# ls -l /test-file
-rw-r--r-- 1 test-user staff 0 Dec 28 18:26 /test-file
root@e877f924ea27:/# exit
exit
docker@boot2docker:~$ ls -l test-file
-rw-r--r-- 1 999 staff 0 Dec 28 18:26 test-file

Build this image as normal and test the new user setting:
$ docker build -t identidock .
...
$ docker run identidock whoami
uwsgi
Note we’ve overridden the default CMD instruction that calls the webserver with the
whoami command, which returns the name of the running user inside the container.


Always Set a USER
It’s important to set the USER statement in all your Dockerfiles (or
change the user within an ENTRYPOINT or CMD script). If you don’t
do this, your processes will be running as root within the container.
As UIDs are the same within a container and on the host, should
an attacker manage to break the container, he will have root access
to the host machine.

Great, now commands inside the container are no longer running as root. Let’s
launch the container again, but with a slightly different set of arguments:
$ docker run -d -P --name port-test identidock
This time we haven’t specified specific ports on the host to bind to. Instead, we’ve
used the -P argument, which makes Docker automatically map a random high-
numbered port on the host to each “exposed” port on the container. We have to ask
Docker what these ports are before we can access the service:
$ docker port port-test
9090/tcp -> 0.0.0.0:32769
9191/tcp -> 0.0.0.0:32768
Here we can see that it has bound 9090 to 32769 on the host and 9191 to 32768, so we
can now access the service (note that the port numbers are likely to be different for
you):
$ curl localhost:32769
Hello Docker!
At first this might seem a pointless extra step—and it is in this case—but when you
have multiple containers running on a single host, it’s a lot easier to ask Docker to
automatically map free ports than it is keep track of unused ports yourself.
So now we have a webservice running that is pretty close to how it would look in pro‐
duction. There are still a lot of things you would want to tweak in production—such
as the uWSGI options for processes and threads—but we have closed the gap enor‐
mously from the default Python debug webserver.
We now have a new problem: we’ve lost access to the development tools such as
debugging output and live code-reloading provided by the default Python web server.
While we can drastically reduce the differences between the development and pro‐
duction environments, they still have fundamentally different needs that will always
require some changes. Ideally, we want to use the same image for both development
and production but enable a slightly different set of features depending on where it is
running. We can achieve this by using an environment variable and a simple script to
switch features depending on context.
Create a file called cmd.sh in the same directory as the Dockerfile with the following
contents:
#!/bin/bash
set -e
if [ "$ENV" = 'DEV' ]; then
  echo "Running Development Server"
  exec python "identidock.py"
else
  echo "Running Production Server"
  exec uwsgi --http 0.0.0.0:9090 --wsgi-file /app/identidock.py --callable app --stats 0.0.0.0:9191
fi
The intent of this script should be fairly clear. If the variable ENV is set to DEV , it will
run the debug webserver; otherwise it will use the production server. 2 . The exec com‐
mand is used in order to avoid creating a new process, which ensures any signals
(such as SIGTERM ) are recieved by our uwsgi process rather than being swallowed by
the parent process.

Use Configuration Files and Helper Scripts
To keep things simple, I’ve included everything inside the Docker‐
file. However, as the application grows, it makes sense to move
things out into supporting files and scripts where possible. In par‐
ticular, the pip dependencies should be moved to a requirements.txt
file, and the uWSGI configuration can move to a .ini file.

Next, we need to update the Dockerfile to use the script:
FROM python:3.4
RUN groupadd -r uwsgi && useradd -r -g uwsgi uwsgi
RUN pip install Flask==0.10.1 uWSGI==2.0.8
WORKDIR /app
COPY app /app
COPY cmd.sh / # ----> Adds the script to the container.
EXPOSE 9090 9191
USER uwsgi
CMD ["/cmd.sh"] # ----> Calls it from the CMD instruction.

Before we try out the new version, it’s time to stop any old containers we have run‐
ning. The following will stop and remove all containers from the host; do not run this
if you have containers you want to keep:
$ docker stop $(docker ps -q)
c4b3d240f187
9be42abaf902
78af7d12d3bb
$ docker rm $(docker ps -aq)
1198f8486390
c4b3d240f187
9be42abaf902
78af7d12d3bb
Now we can rebuild the image with the script and test it out:
$ chmod +x cmd.sh
$ docker build -t identidock .
...
$ docker run -e "ENV=DEV" -p 5000:5000 identidock
unning Development Server
* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
* Restarting with stat
Good. Now when we run with -e "ENV=DEV" , we get a development server; other‐
wise, we get the production server.

Development Servers
You may find that the default Python server doesn’t meet your
needs during development, especially when linking several con‐
tainers together. In this case, you can run uWSGI in development
as well. You will still want the ability to switch environments so that
you can turn on uWSGI features such as live code-reloading, which
shouldn’t be run in production.

Automating with Compose
There’s a final bit of automation we can add to make things a bit simpler. Docker
Compose is designed to quickly get Docker development environments up and run‐
ning. Essentially, it uses YAML files to store the configuration for sets of containers,
saving developers from repetitive and error-prone typing or rolling thier own solu‐
tion. Our application is so basic that it doesn’t buy us much at the moment, but it will
quickly come into its own as things get more complicated. Compose will free us from
the need to maintain our own scripts for orchestration, including starting, linking,
updating, and stopping our containers.
If you installed Docker using the Docker Toolbox, you should already have Compose
available. If not, follow the instructions at the Docker website. I used version 1.4.0 of
Compose in this chapter, but as we’re only using basic functionality, anything after 1.2
should be good.
Create a file called docker-compose.yml in the identidock directory with the following contents:
identidock:  # ------> (1)
build: . # ----> (2)
ports: # ----> (3)
- "5000:5000"
environment: # ----> (4)
ENV: DEV
volumes: # ----> (5)
- ./app:/app

(1) The first line declares the name of the container to build. Multiple containers
(often called services in Compose lingo) can be defined in a single YAML file.
(2) The build key tells Compose that the image for this container is to be built from
a Dockerfile that exists in the current directory ( . ). Every container definition
needs to include either a build or image key. image keys take the tag or ID of an
image to use for the container, the same as image argument to docker run .
(3) The ports key is directly analagous to the -p argument to docker run for expos‐
ing ports. Here we map port 5000 in the container to port 5000 on the host. Ports
can be specified without quotes, but this is best avoided as it can cause confusion
when YAML parses statements such as 56:56 as a base 60 number.
(4) The environment key is directly analagous to the -e argument to docker run ,
which sets environment variables in the container. Here we are setting ENV to DEV
in order to run the Flask development webserver.
(5) The volumes key is directly analogous to the -v argument to docker run for set‐
ting volumes. Here we are bind mounting the app directory into the container as
before in order to allow us to make changes to the code from the host.

Many more keys can be set in Compose YAML files, normally mapping directly to the
equivalent docker run arguments.
If you now run docker-compose up , you will get almost exactly the same result as the
previous docker run command:
$ docker-compose up
Creating identidock_identidock_1...
Attaching to identidock_identidock_1
identidock_1 | Running Development Server
identidock_1 | * Running on http://0.0.0.0:5000/
identidock_1 | * Restarting with reloader
From another terminal:
$ curl localhost:5000
Hello Docker!
When you’re finished running the application, you can just hit ctrl-c to stop the
container.
To switch to the uWSGI server, we would need to change the environment and ports
keys in the YAML. This can either be done by editing the existing docker-compose.yml
or by creating a new YAML file for production and pointing docker-compose at using
the -f flag or the COMPOSE_FILE environment variable.

The Compose Workflow
The following commands are commonly used when working with Compose. 
up --- Starts all the containers defined in the Compose file and aggregates the log out‐
put. Normally you will want to use the -d argument to run Compose in the back‐
ground.
build ---  Rebuilds any images created from Dockerfiles. The up command will not build
an image unless it doesn’t exist, so use this command whenever you need to
update an image.
ps  --- Provides information on the status of containers managed by Compose.
run --- Spins up a container to run a one-off command. This will also spin up any linked
containers unless the --no-deps argument is given.
logs --- Outputs colored and aggregated logs for the Compose-managed containers.
stop --- Stops containers without removing them.
rm --- Removes stopped containers. Remember to use the -v argument to remove any
Docker-managed volumes.

A normal workflow begins with calling docker-compose up -d to start the applica‐
tion. The docker-compose logs and ps commands can be used to verify the status of
the application and help debugging.
After changes to the code, call docker-compose build followed by docker-compose
up -d . This will build the new image and replace the running container. Note that
Compose will preserve any old volumes from the original containers, which means
that databases and caches persist over containers (this can be confusing, so be care
ful). If you don’t need a new image but have modified the Compose YAML, calling up
-d will cause Compose to replace the container with one with the new settings. If you
want to force Compose to stop and recreate all the containers, use the --force-
recreate flag.
When you’re finished with the application, calling docker-compose stop will halt the
application. The same containers will be restarted if docker-compose start or up is
called, assuming no code has changed. Use docker-compose rm to get rid of them
completely.


Conclusion
We’re now at the stage where we have a working environment and we can begin to
develop our application. We’ve seen:
• How to leverage the official images to quickly create a portable and recreatable
development suite, without installing any tools on the host
• How to use volumes to make dynamic changes to code running in containers
• How to maintain both a production and development environment in a single
container
• How to use Compose to automate the development workflow
Docker has given us a familiar development environment, with all the tools we need;
yet at the same time, we can quickly test things out in an environment that mirrors
production.
There’s still a lot of things we need to do, especially with regard to testing and contin‐
uous integration/delivery, but we’ll come to those in the next few chapters as we pro‐
gress with development.



